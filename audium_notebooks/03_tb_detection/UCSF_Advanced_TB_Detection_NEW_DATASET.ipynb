{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced TB Detection Algorithm - NEW DATASET\n",
    "\n",
    "This notebook implements an advanced TB detection algorithm using the new UCSF TB dataset.\n",
    "**This version works with the new dataset structure and embeddings.**\n",
    "\n",
    "## Key Improvements:\n",
    "1. **Temporal Feature Engineering**: Extract temporal patterns from multi-clip embeddings\n",
    "2. **Advanced Data Augmentation**: SMOTE for class balance\n",
    "3. **Patient-Level Aggregation**: Voting across multiple audio files per patient\n",
    "4. **Ensemble Methods**: Combine multiple models for robustness\n",
    "5. **Threshold Optimization**: Optimize for clinical sensitivity requirements\n",
    "\n",
    "## Dataset Information:\n",
    "- **Source**: UCSF TB Project R2D2 lung sounds\n",
    "- **Countries**: Philippines (PH), India (IN), Uganda (UG), Vietnam (VN), South Africa (SA)\n",
    "- **Embedding Model**: Google HeAR (Health Acoustic Representations)\n",
    "- **Embedding Dimension**: 512\n",
    "- **Temporal Clips**: ~10 clips per audio file (2-second clips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Enhanced Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Advanced ML libraries loaded successfully\n",
      "🔧 XGBoost available: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced ML imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, \n",
    "    VotingClassifier, AdaBoostClassifier\n",
    ")\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, roc_curve, auc,\n",
    "    precision_recall_curve, f1_score, fbeta_score, roc_auc_score,\n",
    "    accuracy_score, precision_score, recall_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Data augmentation\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Feature engineering\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "from scipy import stats\n",
    "\n",
    "# XGBoost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "print(\"✅ Advanced ML libraries loaded successfully\")\n",
    "print(f\"🔧 XGBoost available: {XGBOOST_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Data Loading with Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_advanced_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m METADATA_PATH = \u001b[33m\"\u001b[39m\u001b[33mucsf_new_embeddings_metadata.csv\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# UPDATED: Full dataset\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m X, y, file_keys, patient_ids = \u001b[43mload_advanced_embeddings\u001b[49m(\n\u001b[32m      7\u001b[39m     EMBEDDING_PATH, METADATA_PATH, use_temporal=\u001b[38;5;28;01mTrue\u001b[39;00m, max_samples=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🎯 Enhanced dataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🎯 Feature expansion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features (was 512)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_advanced_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the new dataset with temporal features\n",
    "EMBEDDING_PATH = \"ucsf_new_embeddings.npz\"  # UPDATED: Full dataset\n",
    "METADATA_PATH = \"ucsf_new_embeddings_metadata.csv\"  # UPDATED: Full dataset\n",
    "\n",
    "# Load dataset\n",
    "X, y, file_keys, patient_ids = load_advanced_embeddings(\n",
    "    EMBEDDING_PATH, METADATA_PATH, use_temporal=True, max_samples=None\n",
    ")\n",
    "\n",
    "print(f\"\\n🎯 Enhanced dataset shape: {X.shape}\")\n",
    "print(f\"🎯 Feature expansion: {X.shape[1]} features (was 512)\")\n",
    "\n",
    "# Additional dataset summary\n",
    "print(f\"\\n📊 FULL DATASET SUMMARY:\")\n",
    "print(f\"   Total audio files: {len(X)}\")\n",
    "print(f\"   Total patients: {len(np.unique(patient_ids))}\")\n",
    "print(f\"   Average files per patient: {len(X) / len(np.unique(patient_ids)):.1f}\")\n",
    "print(f\"   TB Positive files: {sum(y)} ({sum(y)/len(y)*100:.1f}%)\")\n",
    "print(f\"   TB Negative files: {len(y)-sum(y)} ({(len(y)-sum(y))/len(y)*100:.1f}%)\")\n",
    "print(f\"   Countries: {len(np.unique([pid.split('_')[0] if '_' in pid else pid[:3] for pid in patient_ids]))}\")\n",
    "print(f\"   Temporal features: {X.shape[1]} (13x expansion from 512)\")\n",
    "\n",
    "# Check for any data quality issues\n",
    "print(f\"\\n🔍 DATA QUALITY CHECKS:\")\n",
    "print(f\"   NaN values: {np.isnan(X).sum()}\")\n",
    "print(f\"   Infinite values: {np.isinf(X).sum()}\")\n",
    "print(f\"   Zero variance features: {np.sum(np.var(X, axis=0) == 0)}\")\n",
    "print(f\"   Feature range: [{np.min(X):.3f}, {np.max(X):.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Data Preprocessing and Patient-Level Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patient_level_split(X, y, patient_ids, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Create train/test split ensuring patients don't appear in both sets\n",
    "    \"\"\"\n",
    "    unique_patients = np.unique(patient_ids)\n",
    "    \n",
    "    # Calculate patient-level labels (any TB positive file makes patient positive)\n",
    "    patient_labels = {}\n",
    "    for patient in unique_patients:\n",
    "        patient_mask = patient_ids == patient\n",
    "        patient_labels[patient] = int(np.any(y[patient_mask]))\n",
    "    \n",
    "    # Split patients\n",
    "    patients_array = np.array(list(patient_labels.keys()))\n",
    "    labels_array = np.array(list(patient_labels.values()))\n",
    "    \n",
    "    # Only do stratified split if we have both classes\n",
    "    if len(np.unique(labels_array)) > 1:\n",
    "        train_patients, test_patients = train_test_split(\n",
    "            patients_array, test_size=test_size, stratify=labels_array, random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        train_patients, test_patients = train_test_split(\n",
    "            patients_array, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "    \n",
    "    # Create file-level splits\n",
    "    train_mask = np.isin(patient_ids, train_patients)\n",
    "    test_mask = np.isin(patient_ids, test_patients)\n",
    "    \n",
    "    return (\n",
    "        X[train_mask], X[test_mask],\n",
    "        y[train_mask], y[test_mask],\n",
    "        patient_ids[train_mask], patient_ids[test_mask]\n",
    "    )\n",
    "\n",
    "# Patient-level split\n",
    "X_train, X_test, y_train, y_test, train_patients, test_patients = create_patient_level_split(\n",
    "    X, y, patient_ids, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"🔄 Patient-level split completed\")\n",
    "print(f\"📊 Train: {len(X_train)} files from {len(np.unique(train_patients))} patients\")\n",
    "print(f\"📊 Test: {len(X_test)} files from {len(np.unique(test_patients))} patients\")\n",
    "print(f\"📈 Train TB rate: {sum(y_train)/len(y_train)*100:.1f}%\")\n",
    "print(f\"📈 Test TB rate: {sum(y_test)/len(y_test)*100:.1f}%\")\n",
    "\n",
    "# Apply data augmentation\n",
    "print(\"\\n🔄 Applying advanced data augmentation...\")\n",
    "\n",
    "# Remove features with zero variance\n",
    "var_selector = VarianceThreshold(threshold=0.001)\n",
    "X_train_filtered = var_selector.fit_transform(X_train)\n",
    "X_test_filtered = var_selector.transform(X_test)\n",
    "\n",
    "print(f\"📊 Features after variance filtering: {X_train_filtered.shape[1]} (was {X_train.shape[1]})\")\n",
    "\n",
    "# Apply SMOTE for class balancing only if we have both classes\n",
    "if len(np.unique(y_train)) > 1 and np.sum(y_train) > 1:\n",
    "    # Use k_neighbors based on minority class size\n",
    "    min_samples = min(np.sum(y_train), len(y_train) - np.sum(y_train))\n",
    "    k_neighbors = min(5, min_samples - 1)\n",
    "    \n",
    "    if k_neighbors > 0:\n",
    "        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "        X_train_balanced, y_train_balanced = smote.fit_resample(X_train_filtered, y_train)\n",
    "        \n",
    "        print(f\"✅ SMOTE applied:\")\n",
    "        print(f\"   Before: {Counter(y_train)}\")\n",
    "        print(f\"   After: {Counter(y_train_balanced)}\")\n",
    "        print(f\"   Training set size: {len(X_train_balanced)}\")\n",
    "    else:\n",
    "        print(\"⚠️ Not enough samples for SMOTE, using original data\")\n",
    "        X_train_balanced, y_train_balanced = X_train_filtered, y_train\n",
    "else:\n",
    "    print(\"⚠️ Only one class in training set, skipping SMOTE\")\n",
    "    X_train_balanced, y_train_balanced = X_train_filtered, y_train\n",
    "\n",
    "# Feature scaling\n",
    "scaler = RobustScaler()  # More robust to outliers than StandardScaler\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_test_scaled = scaler.transform(X_test_filtered)\n",
    "\n",
    "print(f\"✅ Feature scaling completed\")\n",
    "\n",
    "# Feature selection\n",
    "selector = SelectKBest(score_func=f_classif, k=min(1000, X_train_scaled.shape[1]))\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train_balanced)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "print(f\"✅ Feature selection: {X_train_selected.shape[1]} features selected\")\n",
    "\n",
    "# Store original test data for patient-level evaluation\n",
    "X_test_original = X_test_filtered\n",
    "y_test_original = y_test\n",
    "test_patients_original = test_patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate advanced class weights\n",
    "pos_weight = len(y_train_balanced[y_train_balanced == 0]) / len(y_train_balanced[y_train_balanced == 1]) if len(y_train_balanced[y_train_balanced == 1]) > 0 else 1.0\n",
    "print(f\"📊 Positive class weight: {pos_weight:.2f}\")\n",
    "\n",
    "# Define advanced models\n",
    "advanced_models = {\n",
    "    \"Optimized SVM\": SVC(\n",
    "        kernel='rbf',\n",
    "        C=1.0,\n",
    "        gamma='scale',\n",
    "        probability=True,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    \"Logistic Regression L1\": LogisticRegression(\n",
    "        penalty='l1',\n",
    "        solver='liblinear',\n",
    "        C=0.1,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    \"Random Forest Balanced\": RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \n",
    "    \"Gradient Boosting Custom\": GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        min_samples_split=5,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    \"Neural Network\": MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32),  # Reduced for small dataset\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.001,\n",
    "        learning_rate='adaptive',\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Add XGBoost if available\n",
    "if XGBOOST_AVAILABLE:\n",
    "    advanced_models[\"XGBoost Optimized\"] = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=pos_weight,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "\n",
    "print(f\"🤖 Configured {len(advanced_models)} advanced models\")\n",
    "for name in advanced_models.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train advanced models\n",
    "trained_advanced_models = {}\n",
    "cv_scores = {}\n",
    "\n",
    "print(\"🚀 Training advanced models...\\n\")\n",
    "\n",
    "for name, model in advanced_models.items():\n",
    "    print(f\"🔄 Training: {name}\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_selected, y_train_balanced)\n",
    "    trained_advanced_models[name] = model\n",
    "    \n",
    "    # Quick cross-validation (reduced folds for small dataset)\n",
    "    try:\n",
    "        if len(np.unique(y_train_balanced)) > 1:\n",
    "            cv_scores_model = cross_val_score(\n",
    "                model, X_train_selected, y_train_balanced, \n",
    "                cv=min(3, len(X_train_selected)), scoring='f1', n_jobs=-1\n",
    "            )\n",
    "            cv_scores[name] = cv_scores_model\n",
    "            print(f\"  ✅ CV F1-Score: {cv_scores_model.mean():.3f} (±{cv_scores_model.std():.3f})\")\n",
    "        else:\n",
    "            print(f\"  ⚠️ CV skipped: only one class in training set\")\n",
    "            cv_scores[name] = [0.0]\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ CV failed: {e}\")\n",
    "        cv_scores[name] = [0.0]\n",
    "    \n",
    "    print(f\"  ✅ Training accuracy: {model.score(X_train_selected, y_train_balanced):.3f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"🎯 All {len(trained_advanced_models)} advanced models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble models\n",
    "print(\"🔄 Creating ensemble models...\")\n",
    "\n",
    "# Select best performing models for ensemble\n",
    "best_models = [\n",
    "    ('svm', advanced_models['Optimized SVM']),\n",
    "    ('lr', advanced_models['Logistic Regression L1']),\n",
    "    ('rf', advanced_models['Random Forest Balanced'])\n",
    "]\n",
    "\n",
    "# Voting classifier (soft voting for probabilities)\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=best_models,\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Train ensemble\n",
    "print(\"🔄 Training ensemble...\")\n",
    "voting_clf.fit(X_train_selected, y_train_balanced)\n",
    "\n",
    "# Add to models\n",
    "trained_advanced_models['Ensemble (Voting)'] = voting_clf\n",
    "\n",
    "print(\"✅ Ensemble model trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Evaluation with Patient-Level Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_advanced_model(model, X_test, y_test, test_patients, model_name):\n",
    "    \"\"\"\n",
    "    Advanced evaluation with both file-level and patient-level metrics\n",
    "    \"\"\"\n",
    "    # File-level predictions\n",
    "    y_pred_file = model.predict(X_test)\n",
    "    \n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_prob_file = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_prob_file = y_pred_file\n",
    "    \n",
    "    # Patient-level aggregation\n",
    "    unique_patients = np.unique(test_patients)\n",
    "    patient_predictions = []\n",
    "    patient_true_labels = []\n",
    "    patient_probs = []\n",
    "    \n",
    "    for patient in unique_patients:\n",
    "        patient_mask = test_patients == patient\n",
    "        patient_files_pred = y_pred_file[patient_mask]\n",
    "        patient_files_true = y_test[patient_mask]\n",
    "        patient_files_prob = y_prob_file[patient_mask]\n",
    "        \n",
    "        # Patient-level aggregation strategies\n",
    "        # 1. Any positive file makes patient positive (sensitive)\n",
    "        patient_pred_any = int(np.any(patient_files_pred))\n",
    "        patient_true_any = int(np.any(patient_files_true))\n",
    "        patient_prob_max = np.max(patient_files_prob)\n",
    "        \n",
    "        patient_predictions.append(patient_pred_any)\n",
    "        patient_true_labels.append(patient_true_any)\n",
    "        patient_probs.append(patient_prob_max)\n",
    "    \n",
    "    patient_predictions = np.array(patient_predictions)\n",
    "    patient_true_labels = np.array(patient_true_labels)\n",
    "    patient_probs = np.array(patient_probs)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    # File-level metrics\n",
    "    cm_file = confusion_matrix(y_test, y_pred_file)\n",
    "    if cm_file.shape == (2, 2):\n",
    "        tn_f, fp_f, fn_f, tp_f = cm_file.ravel()\n",
    "    else:\n",
    "        tn_f, fp_f, fn_f, tp_f = 0, 0, 0, 0\n",
    "    \n",
    "    # Patient-level metrics\n",
    "    cm_patient = confusion_matrix(patient_true_labels, patient_predictions)\n",
    "    if cm_patient.shape == (2, 2):\n",
    "        tn_p, fp_p, fn_p, tp_p = cm_patient.ravel()\n",
    "    else:\n",
    "        tn_p, fp_p, fn_p, tp_p = 0, 0, 0, 0\n",
    "    \n",
    "    # Calculate clinical metrics\n",
    "    def safe_divide(a, b):\n",
    "        return a / b if b > 0 else 0\n",
    "    \n",
    "    # File-level metrics\n",
    "    file_metrics = {\n",
    "        'sensitivity': safe_divide(tp_f, tp_f + fn_f),\n",
    "        'specificity': safe_divide(tn_f, tn_f + fp_f),\n",
    "        'precision': safe_divide(tp_f, tp_f + fp_f),\n",
    "        'npv': safe_divide(tn_f, tn_f + fn_f),\n",
    "        'f1': f1_score(y_test, y_pred_file, average='weighted') if len(np.unique(y_test)) > 1 else 0,\n",
    "        'f2': fbeta_score(y_test, y_pred_file, beta=2, average='weighted') if len(np.unique(y_test)) > 1 else 0,\n",
    "        'accuracy': accuracy_score(y_test, y_pred_file)\n",
    "    }\n",
    "    \n",
    "    # Patient-level metrics\n",
    "    patient_metrics = {\n",
    "        'sensitivity': safe_divide(tp_p, tp_p + fn_p),\n",
    "        'specificity': safe_divide(tn_p, tn_p + fp_p),\n",
    "        'precision': safe_divide(tp_p, tp_p + fp_p),\n",
    "        'npv': safe_divide(tn_p, tn_p + fn_p),\n",
    "        'f1': f1_score(patient_true_labels, patient_predictions, average='weighted') if len(np.unique(patient_true_labels)) > 1 else 0,\n",
    "        'f2': fbeta_score(patient_true_labels, patient_predictions, beta=2, average='weighted') if len(np.unique(patient_true_labels)) > 1 else 0,\n",
    "        'accuracy': accuracy_score(patient_true_labels, patient_predictions)\n",
    "    }\n",
    "    \n",
    "    # AUC metrics\n",
    "    try:\n",
    "        if len(np.unique(y_test)) > 1:\n",
    "            file_roc_auc = roc_auc_score(y_test, y_prob_file)\n",
    "            precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_prob_file)\n",
    "            file_pr_auc = auc(recall_vals, precision_vals)\n",
    "        else:\n",
    "            file_roc_auc = file_pr_auc = 0.0\n",
    "            \n",
    "        if len(np.unique(patient_true_labels)) > 1:\n",
    "            patient_roc_auc = roc_auc_score(patient_true_labels, patient_probs)\n",
    "            precision_vals_p, recall_vals_p, _ = precision_recall_curve(patient_true_labels, patient_probs)\n",
    "            patient_pr_auc = auc(recall_vals_p, precision_vals_p)\n",
    "        else:\n",
    "            patient_roc_auc = patient_pr_auc = 0.0\n",
    "    except:\n",
    "        file_roc_auc = patient_roc_auc = file_pr_auc = patient_pr_auc = 0.0\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'file_metrics': file_metrics,\n",
    "        'patient_metrics': patient_metrics,\n",
    "        'file_roc_auc': file_roc_auc,\n",
    "        'patient_roc_auc': patient_roc_auc,\n",
    "        'file_pr_auc': file_pr_auc,\n",
    "        'patient_pr_auc': patient_pr_auc,\n",
    "        'file_cm': cm_file,\n",
    "        'patient_cm': cm_patient,\n",
    "        'file_predictions': y_pred_file,\n",
    "        'patient_predictions': patient_predictions,\n",
    "        'file_probs': y_prob_file,\n",
    "        'patient_probs': patient_probs,\n",
    "        'n_patients': len(unique_patients),\n",
    "        'n_files': len(y_test),\n",
    "        'tp_p': tp_p, 'fn_p': fn_p, 'tn_p': tn_p, 'fp_p': fp_p\n",
    "    }\n",
    "\n",
    "# Evaluate all advanced models\n",
    "advanced_results = {}\n",
    "\n",
    "print(\"📊 Evaluating advanced models...\\n\")\n",
    "\n",
    "for name, model in trained_advanced_models.items():\n",
    "    result = evaluate_advanced_model(\n",
    "        model, X_test_selected, y_test_original, test_patients_original, name\n",
    "    )\n",
    "    advanced_results[name] = result\n",
    "    \n",
    "    print(f\"🔍 {name}:\")\n",
    "    print(f\"  📁 File-level Sensitivity: {result['file_metrics']['sensitivity']:.3f}\")\n",
    "    print(f\"  🏥 Patient-level Sensitivity: {result['patient_metrics']['sensitivity']:.3f}\")\n",
    "    print(f\"  📁 File-level F2-Score: {result['file_metrics']['f2']:.3f}\")\n",
    "    print(f\"  🏥 Patient-level F2-Score: {result['patient_metrics']['f2']:.3f}\")\n",
    "    print(f\"  📊 Patient-level PR-AUC: {result['patient_pr_auc']:.3f}\")\n",
    "    print(f\"  🎯 Clinical Target (≥80%): {'✅' if result['patient_metrics']['sensitivity'] >= 0.8 else '❌'}\")\n",
    "    print(f\"  🏥 TB Patients Detected: {result['tp_p']}/{result['tp_p'] + result['fn_p']}\")\n",
    "    print()\n",
    "\n",
    "print(\"✅ Advanced evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results summary\n",
    "print(\"📋 NEW DATASET RESULTS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for name, result in advanced_results.items():\n",
    "    summary_data.append({\n",
    "        'Model': name,\n",
    "        'Patient Sensitivity': f\"{result['patient_metrics']['sensitivity']:.3f}\",\n",
    "        'Patient Specificity': f\"{result['patient_metrics']['specificity']:.3f}\",\n",
    "        'Patient Precision': f\"{result['patient_metrics']['precision']:.3f}\",\n",
    "        'Patient F2-Score': f\"{result['patient_metrics']['f2']:.3f}\",\n",
    "        'Patient PR-AUC': f\"{result['patient_pr_auc']:.3f}\",\n",
    "        'Patient ROC-AUC': f\"{result['patient_roc_auc']:.3f}\",\n",
    "        'Clinical Target': '✅' if result['patient_metrics']['sensitivity'] >= 0.8 else '❌',\n",
    "        'TB Patients Detected': f\"{result['tp_p']}/{result['tp_p'] + result['fn_p']}\"\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(summary_data)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Dataset summary\n",
    "print(f\"\\n📊 DATASET SUMMARY:\")\n",
    "print(f\"   Total samples: {len(X)}\")\n",
    "print(f\"   Total patients: {len(np.unique(patient_ids))}\")\n",
    "print(f\"   TB Positive: {sum(y)} ({sum(y)/len(y)*100:.1f}%)\")\n",
    "print(f\"   TB Negative: {len(y)-sum(y)} ({(len(y)-sum(y))/len(y)*100:.1f}%)\")\n",
    "print(f\"   Features: {X.shape[1]} (temporal expansion from 512)\")\n",
    "\n",
    "# Best model\n",
    "if len(advanced_results) > 0:\n",
    "    best_model = max(advanced_results.items(), key=lambda x: x[1]['patient_metrics']['sensitivity'])\n",
    "    print(f\"\\n🏆 BEST MODEL: {best_model[0]}\")\n",
    "    print(f\"   Patient-level Sensitivity: {best_model[1]['patient_metrics']['sensitivity']:.3f}\")\n",
    "    print(f\"   Patient-level Specificity: {best_model[1]['patient_metrics']['specificity']:.3f}\")\n",
    "    print(f\"   Patient-level Precision: {best_model[1]['patient_metrics']['precision']:.3f}\")\n",
    "    print(f\"   Patient-level F2-Score: {best_model[1]['patient_metrics']['f2']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"🎉 NEW DATASET ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comprehensive Visualizations\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\\\"husl\\\")\n",
    "\n",
    "def plot_confusion_matrices(advanced_results, figsize=(20, 12)):\n",
    "    \\\"\\\"\\\"Plot confusion matrices for all models\\\"\\\"\\\"\n",
    "    n_models = len(advanced_results)\n",
    "    fig, axes = plt.subplots(2, n_models, figsize=figsize)\n",
    "    \n",
    "    if n_models == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    for i, (name, result) in enumerate(advanced_results.items()):\n",
    "        # File-level confusion matrix\n",
    "        ax1 = axes[0, i]\n",
    "        cm_file = result['file_cm']\n",
    "        if cm_file.size > 0:\n",
    "            sns.heatmap(cm_file, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "                       xticklabels=['TB-', 'TB+'], yticklabels=['TB-', 'TB+'])\n",
    "            ax1.set_title(f'{name}\\\\nFile-level Confusion Matrix')\n",
    "            ax1.set_xlabel('Predicted')\n",
    "            ax1.set_ylabel('Actual')\n",
    "        \n",
    "        # Patient-level confusion matrix\n",
    "        ax2 = axes[1, i]\n",
    "        cm_patient = result['patient_cm']\n",
    "        if cm_patient.size > 0:\n",
    "            sns.heatmap(cm_patient, annot=True, fmt='d', cmap='Oranges', ax=ax2,\n",
    "                       xticklabels=['TB-', 'TB+'], yticklabels=['TB-', 'TB+'])\n",
    "            ax2.set_title(f'{name}\\\\nPatient-level Confusion Matrix')\n",
    "            ax2.set_xlabel('Predicted')\n",
    "            ax2.set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curves(advanced_results, figsize=(15, 6)):\n",
    "    \\\"\\\"\\\"Plot ROC curves for all models\\\"\\\"\\\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # File-level ROC curves\n",
    "    for name, result in advanced_results.items():\n",
    "        if result['file_roc_auc'] > 0:\n",
    "            # Calculate ROC curve points\n",
    "            y_true = np.concatenate([np.ones(result['tp_p']), np.zeros(result['tn_p']), \n",
    "                                   np.ones(result['fn_p']), np.zeros(result['fp_p'])])\n",
    "            y_scores = result['file_probs']\n",
    "            \n",
    "            if len(np.unique(y_true)) > 1:\n",
    "                fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "                ax1.plot(fpr, tpr, label=f'{name} (AUC={result[\\\"file_roc_auc\\\"]:.3f})')\n",
    "    \n",
    "    ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title('File-level ROC Curves')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Patient-level ROC curves\n",
    "    for name, result in advanced_results.items():\n",
    "        if result['patient_roc_auc'] > 0:\n",
    "            # Use patient-level data\n",
    "            y_true_patient = np.concatenate([np.ones(result['tp_p']), np.zeros(result['tn_p']), \n",
    "                                           np.ones(result['fn_p']), np.zeros(result['fp_p'])])\n",
    "            y_scores_patient = result['patient_probs']\n",
    "            \n",
    "            if len(np.unique(y_true_patient)) > 1 and len(y_scores_patient) > 0:\n",
    "                fpr, tpr, _ = roc_curve(y_true_patient, y_scores_patient)\n",
    "                ax2.plot(fpr, tpr, label=f'{name} (AUC={result[\\\"patient_roc_auc\\\"]:.3f})')\n",
    "    \n",
    "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax2.set_xlabel('False Positive Rate')\n",
    "    ax2.set_ylabel('True Positive Rate')\n",
    "    ax2.set_title('Patient-level ROC Curves')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall_curves(advanced_results, figsize=(15, 6)):\n",
    "    \\\"\\\"\\\"Plot Precision-Recall curves for all models\\\"\\\"\\\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # File-level PR curves\n",
    "    for name, result in advanced_results.items():\n",
    "        if result['file_pr_auc'] > 0:\n",
    "            y_true = np.concatenate([np.ones(result['tp_p']), np.zeros(result['tn_p']), \n",
    "                                   np.ones(result['fn_p']), np.zeros(result['fp_p'])])\n",
    "            y_scores = result['file_probs']\n",
    "            \n",
    "            if len(np.unique(y_true)) > 1:\n",
    "                precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "                ax1.plot(recall, precision, label=f'{name} (AUC={result[\\\"file_pr_auc\\\"]:.3f})')\n",
    "    \n",
    "    ax1.set_xlabel('Recall')\n",
    "    ax1.set_ylabel('Precision')\n",
    "    ax1.set_title('File-level Precision-Recall Curves')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Patient-level PR curves\n",
    "    for name, result in advanced_results.items():\n",
    "        if result['patient_pr_auc'] > 0:\n",
    "            y_true_patient = np.concatenate([np.ones(result['tp_p']), np.zeros(result['tn_p']), \n",
    "                                           np.ones(result['fn_p']), np.zeros(result['fp_p'])])\n",
    "            y_scores_patient = result['patient_probs']\n",
    "            \n",
    "            if len(np.unique(y_true_patient)) > 1 and len(y_scores_patient) > 0:\n",
    "                precision, recall, _ = precision_recall_curve(y_true_patient, y_scores_patient)\n",
    "                ax2.plot(recall, precision, label=f'{name} (AUC={result[\\\"patient_pr_auc\\\"]:.3f})')\n",
    "    \n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_title('Patient-level Precision-Recall Curves')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_performance_metrics(advanced_results, figsize=(15, 10)):\n",
    "    \\\"\\\"\\\"Plot performance metrics comparison\\\"\\\"\\\"\n",
    "    models = list(advanced_results.keys())\n",
    "    \n",
    "    # Extract metrics\n",
    "    file_metrics = {\n",
    "        'Sensitivity': [advanced_results[m]['file_metrics']['sensitivity'] for m in models],\n",
    "        'Specificity': [advanced_results[m]['file_metrics']['specificity'] for m in models],\n",
    "        'Precision': [advanced_results[m]['file_metrics']['precision'] for m in models],\n",
    "        'F2-Score': [advanced_results[m]['file_metrics']['f2'] for m in models],\n",
    "        'Accuracy': [advanced_results[m]['file_metrics']['accuracy'] for m in models]\n",
    "    }\n",
    "    \n",
    "    patient_metrics = {\n",
    "        'Sensitivity': [advanced_results[m]['patient_metrics']['sensitivity'] for m in models],\n",
    "        'Specificity': [advanced_results[m]['patient_metrics']['specificity'] for m in models],\n",
    "        'Precision': [advanced_results[m]['patient_metrics']['precision'] for m in models],\n",
    "        'F2-Score': [advanced_results[m]['patient_metrics']['f2'] for m in models],\n",
    "        'Accuracy': [advanced_results[m]['patient_metrics']['accuracy'] for m in models]\n",
    "    }\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize)\n",
    "    \n",
    "    # File-level metrics\n",
    "    file_df = pd.DataFrame(file_metrics, index=models)\n",
    "    file_df.plot(kind='bar', ax=ax1, width=0.8)\n",
    "    ax1.set_title('File-level Performance Metrics')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add clinical target line for sensitivity\n",
    "    ax1.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Clinical Target (80%)')\n",
    "    \n",
    "    # Patient-level metrics\n",
    "    patient_df = pd.DataFrame(patient_metrics, index=models)\n",
    "    patient_df.plot(kind='bar', ax=ax2, width=0.8)\n",
    "    ax2.set_title('Patient-level Performance Metrics')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add clinical target line for sensitivity\n",
    "    ax2.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Clinical Target (80%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate all visualizations\n",
    "if len(advanced_results) > 0:\n",
    "    print(\\\"\\\\n\\\" + \\\"=\\\" * 60)\n",
    "    print(\\\"📊 COMPREHENSIVE VISUALIZATIONS\\\")\n",
    "    print(\\\"=\\\" * 60)\n",
    "    \n",
    "    # 1. Confusion Matrices\n",
    "    print(\\\"\\\\n1️⃣ Confusion Matrices (File-level and Patient-level)\\\")\n",
    "    plot_confusion_matrices(advanced_results)\n",
    "    \n",
    "    # 2. ROC Curves\n",
    "    print(\\\"\\\\n2️⃣ ROC Curves (File-level and Patient-level)\\\")\n",
    "    plot_roc_curves(advanced_results)\n",
    "    \n",
    "    # 3. Precision-Recall Curves\n",
    "    print(\\\"\\\\n3️⃣ Precision-Recall Curves (File-level and Patient-level)\\\")\n",
    "    plot_precision_recall_curves(advanced_results)\n",
    "    \n",
    "    # 4. Performance Metrics Comparison\n",
    "    print(\\\"\\\\n4️⃣ Performance Metrics Comparison\\\")\n",
    "    plot_performance_metrics(advanced_results)\n",
    "    \n",
    "    print(\\\"\\\\n\\\" + \\\"=\\\" * 60)\n",
    "    print(\\\"📈 ALL VISUALIZATIONS COMPLETE\\\")\n",
    "    print(\\\"=\\\" * 60)\n",
    "else:\n",
    "    print(\\\"⚠️ No results available for visualization\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (3139027568.py, line 4)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\\\"\\\"\\\"Create a comprehensive metrics table\\\"\\\"\\\"\u001b[39m\n     ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "## Detailed Performance Metrics Table\n",
    "\n",
    "def create_detailed_metrics_table(advanced_results):\n",
    "    \\\"\\\"\\\"Create a comprehensive metrics table\\\"\\\"\\\"\n",
    "    detailed_data = []\n",
    "    \n",
    "    for name, result in advanced_results.items():\n",
    "        # File-level metrics\n",
    "        detailed_data.append({\n",
    "            'Model': name,\n",
    "            'Level': 'File',\n",
    "            'Sensitivity (Recall)': f\\\"{result['file_metrics']['sensitivity']:.3f}\\\",\n",
    "            'Specificity': f\\\"{result['file_metrics']['specificity']:.3f}\\\",\n",
    "            'Precision (PPV)': f\\\"{result['file_metrics']['precision']:.3f}\\\",\n",
    "            'NPV': f\\\"{result['file_metrics']['npv']:.3f}\\\",\n",
    "            'F1-Score': f\\\"{result['file_metrics']['f1']:.3f}\\\",\n",
    "            'F2-Score': f\\\"{result['file_metrics']['f2']:.3f}\\\",\n",
    "            'Accuracy': f\\\"{result['file_metrics']['accuracy']:.3f}\\\",\n",
    "            'ROC-AUC': f\\\"{result['file_roc_auc']:.3f}\\\",\n",
    "            'PR-AUC': f\\\"{result['file_pr_auc']:.3f}\\\",\n",
    "            'TP': result.get('tp_f', 0),\n",
    "            'TN': result.get('tn_f', 0),\n",
    "            'FP': result.get('fp_f', 0),\n",
    "            'FN': result.get('fn_f', 0)\\n        })\\n        \\n        # Patient-level metrics\\n        detailed_data.append({\\n            'Model': name,\\n            'Level': 'Patient',\\n            'Sensitivity (Recall)': f\\\"{result['patient_metrics']['sensitivity']:.3f}\\\",\\n            'Specificity': f\\\"{result['patient_metrics']['specificity']:.3f}\\\",\\n            'Precision (PPV)': f\\\"{result['patient_metrics']['precision']:.3f}\\\",\\n            'NPV': f\\\"{result['patient_metrics']['npv']:.3f}\\\",\\n            'F1-Score': f\\\"{result['patient_metrics']['f1']:.3f}\\\",\\n            'F2-Score': f\\\"{result['patient_metrics']['f2']:.3f}\\\",\\n            'Accuracy': f\\\"{result['patient_metrics']['accuracy']:.3f}\\\",\\n            'ROC-AUC': f\\\"{result['patient_roc_auc']:.3f}\\\",\\n            'PR-AUC': f\\\"{result['patient_pr_auc']:.3f}\\\",\\n            'TP': result['tp_p'],\\n            'TN': result['tn_p'],\\n            'FP': result['fp_p'],\\n            'FN': result['fn_p']\\n        })\\n    \\n    return pd.DataFrame(detailed_data)\\n\\n# Create and display detailed metrics table\\nif len(advanced_results) > 0:\\n    print(\\\"\\\\n\\\" + \\\"=\\\" * 80)\\n    print(\\\"📋 DETAILED PERFORMANCE METRICS TABLE\\\")\\n    print(\\\"=\\\" * 80)\\n    \\n    detailed_df = create_detailed_metrics_table(advanced_results)\\n    \\n    # Display with better formatting\\n    pd.set_option('display.max_columns', None)\\n    pd.set_option('display.width', None)\\n    pd.set_option('display.max_colwidth', None)\\n    \\n    print(detailed_df.to_string(index=False))\\n    \\n    # Clinical interpretation\\n    print(\\\"\\\\n\\\" + \\\"=\\\" * 80)\\n    print(\\\"🏥 CLINICAL INTERPRETATION\\\")\\n    print(\\\"=\\\" * 80)\\n    \\n    print(\\\"📊 Key Clinical Metrics:\\\")\\n    print(\\\"   • Sensitivity (Recall): Proportion of TB cases correctly identified\\\")\\n    print(\\\"   • Specificity: Proportion of non-TB cases correctly identified\\\")\\n    print(\\\"   • Precision (PPV): Proportion of positive predictions that are correct\\\")\\n    print(\\\"   • NPV: Proportion of negative predictions that are correct\\\")\\n    print(\\\"   • F2-Score: Weighted harmonic mean favoring recall (clinical focus)\\\")\\n    print(\\\"   • Clinical Target: ≥80% sensitivity for TB screening\\\")\\n    \\n    # Find models meeting clinical target\\n    meeting_target = detailed_df[\\n        (detailed_df['Level'] == 'Patient') & \\n        (detailed_df['Sensitivity (Recall)'].astype(float) >= 0.8)\\n    ]\\n    \\n    if len(meeting_target) > 0:\\n        print(f\\\"\\\\n✅ Models meeting clinical target (≥80% sensitivity):\\\")\\n        for _, row in meeting_target.iterrows():\\n            print(f\\\"   • {row['Model']}: {row['Sensitivity (Recall)']} sensitivity\\\")\\n    else:\\n        print(f\\\"\\\\n⚠️ No models currently meet the clinical target of ≥80% sensitivity\\\")\\n        best_sensitivity = detailed_df[\\n            detailed_df['Level'] == 'Patient'\\n        ]['Sensitivity (Recall)'].astype(float).max()\\n        print(f\\\"   • Best sensitivity achieved: {best_sensitivity:.3f}\\\")\\n        print(f\\\"   • Gap to target: {0.8 - best_sensitivity:.3f}\\\")\\n    \\n    print(\\\"\\\\n\\\" + \\\"=\\\" * 80)\\n    print(\\\"📈 METRICS TABLE COMPLETE\\\")\\n    print(\\\"=\\\" * 80)\\nelse:\\n    print(\\\"⚠️ No results available for detailed metrics table\\\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
